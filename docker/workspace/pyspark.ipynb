{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark**: The Apache Spark Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark cluster to process data using Spark Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Connection\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json,col\n",
    "from pyspark.sql.types import *\n",
    "from os.path import abspath\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/workspace/spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "warehouse_location = abspath('spark-warehouse')\n",
    "print(warehouse_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+----------------------------------------------------------------+\n",
      "|key                                                        |value                                                           |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes            |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>|\n",
      "|spark.sql.adaptive.coalescePartitions.enabled              |true                                                            |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum  |<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionNum      |<undefined>                                                     |\n",
      "|spark.sql.adaptive.enabled                                 |false                                                           |\n",
      "|spark.sql.adaptive.localShuffleReader.enabled              |true                                                            |\n",
      "|spark.sql.adaptive.skewJoin.enabled                        |true                                                            |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionFactor          |5                                                               |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes|256MB                                                           |\n",
      "|spark.sql.ansi.enabled                                     |false                                                           |\n",
      "|spark.sql.autoBroadcastJoinThreshold                       |10MB                                                            |\n",
      "|spark.sql.avro.compression.codec                           |snappy                                                          |\n",
      "|spark.sql.avro.deflate.level                               |-1                                                              |\n",
      "|spark.sql.broadcastTimeout                                 |300                                                             |\n",
      "|spark.sql.catalog.spark_catalog                            |<undefined>                                                     |\n",
      "|spark.sql.cbo.enabled                                      |false                                                           |\n",
      "|spark.sql.cbo.joinReorder.dp.star.filter                   |false                                                           |\n",
      "|spark.sql.cbo.joinReorder.dp.threshold                     |12                                                              |\n",
      "|spark.sql.cbo.joinReorder.enabled                          |false                                                           |\n",
      "|spark.sql.cbo.planStats.enabled                            |false                                                           |\n",
      "|spark.sql.cbo.starSchemaDetection                          |false                                                           |\n",
      "|spark.sql.columnNameOfCorruptRecord                        |_corrupt_record                                                 |\n",
      "|spark.sql.csv.filterPushdown.enabled                       |true                                                            |\n",
      "|spark.sql.datetime.java8API.enabled                        |false                                                           |\n",
      "|spark.sql.debug.maxToStringFields                          |25                                                              |\n",
      "|spark.sql.defaultCatalog                                   |spark_catalog                                                   |\n",
      "|spark.sql.event.truncate.length                            |2147483647                                                      |\n",
      "|spark.sql.execution.arrow.enabled                          |false                                                           |\n",
      "|spark.sql.execution.arrow.fallback.enabled                 |true                                                            |\n",
      "|spark.sql.execution.arrow.maxRecordsPerBatch               |10000                                                           |\n",
      "|spark.sql.execution.arrow.pyspark.enabled                  |<value of spark.sql.execution.arrow.enabled>                    |\n",
      "|spark.sql.execution.arrow.pyspark.fallback.enabled         |<value of spark.sql.execution.arrow.fallback.enabled>           |\n",
      "|spark.sql.execution.arrow.sparkr.enabled                   |false                                                           |\n",
      "|spark.sql.execution.pandas.udf.buffer.size                 |<value of spark.buffer.size>                                    |\n",
      "|spark.sql.extensions                                       |<undefined>                                                     |\n",
      "|spark.sql.files.ignoreCorruptFiles                         |false                                                           |\n",
      "|spark.sql.files.ignoreMissingFiles                         |false                                                           |\n",
      "|spark.sql.files.maxPartitionBytes                          |128MB                                                           |\n",
      "|spark.sql.files.maxRecordsPerFile                          |0                                                               |\n",
      "|spark.sql.function.concatBinaryAsString                    |false                                                           |\n",
      "|spark.sql.function.eltOutputAsString                       |false                                                           |\n",
      "|spark.sql.groupByAliases                                   |true                                                            |\n",
      "|spark.sql.groupByOrdinal                                   |true                                                            |\n",
      "|spark.sql.hive.filesourcePartitionFileCacheSize            |262144000                                                       |\n",
      "|spark.sql.hive.manageFilesourcePartitions                  |true                                                            |\n",
      "|spark.sql.hive.metastorePartitionPruning                   |true                                                            |\n",
      "|spark.sql.hive.thriftServer.singleSession                  |false                                                           |\n",
      "|spark.sql.hive.verifyPartitionPath                         |false                                                           |\n",
      "|spark.sql.inMemoryColumnarStorage.batchSize                |10000                                                           |\n",
      "|spark.sql.inMemoryColumnarStorage.compressed               |true                                                            |\n",
      "|spark.sql.inMemoryColumnarStorage.enableVectorizedReader   |true                                                            |\n",
      "|spark.sql.jsonGenerator.ignoreNullFields                   |true                                                            |\n",
      "|spark.sql.legacy.allowHashOnMapType                        |false                                                           |\n",
      "|spark.sql.legacy.sessionInitWithConfigDefaults             |false                                                           |\n",
      "|spark.sql.mapKeyDedupPolicy                                |EXCEPTION                                                       |\n",
      "|spark.sql.maven.additionalRemoteRepositories               |https://maven-central.storage-download.googleapis.com/maven2/   |\n",
      "|spark.sql.maxPlanStringLength                              |2147483632                                                      |\n",
      "|spark.sql.optimizer.dynamicPartitionPruning.enabled        |true                                                            |\n",
      "|spark.sql.optimizer.excludedRules                          |<undefined>                                                     |\n",
      "|spark.sql.orc.columnarReaderBatchSize                      |4096                                                            |\n",
      "|spark.sql.orc.compression.codec                            |snappy                                                          |\n",
      "|spark.sql.orc.enableVectorizedReader                       |true                                                            |\n",
      "|spark.sql.orc.filterPushdown                               |true                                                            |\n",
      "|spark.sql.orc.mergeSchema                                  |false                                                           |\n",
      "|spark.sql.orderByOrdinal                                   |true                                                            |\n",
      "|spark.sql.parquet.binaryAsString                           |false                                                           |\n",
      "|spark.sql.parquet.columnarReaderBatchSize                  |4096                                                            |\n",
      "|spark.sql.parquet.compression.codec                        |snappy                                                          |\n",
      "|spark.sql.parquet.enableVectorizedReader                   |true                                                            |\n",
      "|spark.sql.parquet.filterPushdown                           |true                                                            |\n",
      "|spark.sql.parquet.int96AsTimestamp                         |true                                                            |\n",
      "|spark.sql.parquet.int96TimestampConversion                 |false                                                           |\n",
      "|spark.sql.parquet.mergeSchema                              |false                                                           |\n",
      "|spark.sql.parquet.outputTimestampType                      |INT96                                                           |\n",
      "|spark.sql.parquet.recordLevelFilter.enabled                |false                                                           |\n",
      "|spark.sql.parquet.respectSummaryFiles                      |false                                                           |\n",
      "|spark.sql.parquet.writeLegacyFormat                        |false                                                           |\n",
      "|spark.sql.parser.quotedRegexColumnNames                    |false                                                           |\n",
      "|spark.sql.pivotMaxValues                                   |10000                                                           |\n",
      "|spark.sql.pyspark.jvmStacktrace.enabled                    |false                                                           |\n",
      "|spark.sql.queryExecutionListeners                          |<undefined>                                                     |\n",
      "|spark.sql.redaction.options.regex                          |(?i)url                                                         |\n",
      "|spark.sql.redaction.string.regex                           |<value of spark.redaction.string.regex>                         |\n",
      "|spark.sql.repl.eagerEval.enabled                           |false                                                           |\n",
      "|spark.sql.repl.eagerEval.maxNumRows                        |20                                                              |\n",
      "|spark.sql.repl.eagerEval.truncate                          |20                                                              |\n",
      "|spark.sql.session.timeZone                                 |Etc/UTC                                                         |\n",
      "|spark.sql.shuffle.partitions                               |200                                                             |\n",
      "|spark.sql.sources.bucketing.enabled                        |true                                                            |\n",
      "|spark.sql.sources.bucketing.maxBuckets                     |100000                                                          |\n",
      "|spark.sql.sources.default                                  |parquet                                                         |\n",
      "|spark.sql.sources.parallelPartitionDiscovery.threshold     |32                                                              |\n",
      "|spark.sql.sources.partitionColumnTypeInference.enabled     |true                                                            |\n",
      "|spark.sql.sources.partitionOverwriteMode                   |STATIC                                                          |\n",
      "|spark.sql.statistics.fallBackToHdfs                        |false                                                           |\n",
      "|spark.sql.statistics.histogram.enabled                     |false                                                           |\n",
      "|spark.sql.statistics.size.autoUpdate.enabled               |false                                                           |\n",
      "|spark.sql.storeAssignmentPolicy                            |ANSI                                                            |\n",
      "|spark.sql.streaming.checkpointLocation                     |<undefined>                                                     |\n",
      "|spark.sql.streaming.continuous.epochBacklogQueueSize       |10000                                                           |\n",
      "|spark.sql.streaming.disabledV2Writers                      |                                                                |\n",
      "|spark.sql.streaming.fileSource.cleaner.numThreads          |1                                                               |\n",
      "|spark.sql.streaming.forceDeleteTempCheckpointLocation      |false                                                           |\n",
      "|spark.sql.streaming.metricsEnabled                         |false                                                           |\n",
      "|spark.sql.streaming.multipleWatermarkPolicy                |min                                                             |\n",
      "|spark.sql.streaming.noDataMicroBatches.enabled             |true                                                            |\n",
      "|spark.sql.streaming.numRecentProgressUpdates               |100                                                             |\n",
      "|spark.sql.streaming.stopActiveRunOnRestart                 |true                                                            |\n",
      "|spark.sql.streaming.stopTimeout                            |0                                                               |\n",
      "|spark.sql.streaming.streamingQueryListeners                |<undefined>                                                     |\n",
      "|spark.sql.streaming.ui.enabled                             |true                                                            |\n",
      "|spark.sql.streaming.ui.retainedProgressUpdates             |100                                                             |\n",
      "|spark.sql.streaming.ui.retainedQueries                     |100                                                             |\n",
      "|spark.sql.thriftserver.scheduler.pool                      |<undefined>                                                     |\n",
      "|spark.sql.thriftserver.ui.retainedSessions                 |200                                                             |\n",
      "|spark.sql.thriftserver.ui.retainedStatements               |200                                                             |\n",
      "|spark.sql.ui.retainedExecutions                            |1000                                                            |\n",
      "|spark.sql.variable.substitute                              |true                                                            |\n",
      "|spark.sql.warehouse.dir                                    |/user/hive/warehouse                                            |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET -v\").select(\"key\", \"value\").show(200, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More confs for SparkSession object in standalone mode can be added using the **config** method. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Introduction\n",
    "\n",
    "We will be using Spark Python API to read, process and write data. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Read\n",
    "\n",
    "Let's read some UK's macroeconomic data ([source](https://www.kaggle.com/bank-of-england/a-millennium-of-macroeconomic-data)) from the cluster's simulated **Hadoop distributed file system (HDFS)** into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(path=\"datasets/uk-macroeconomic-data.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = data.select([\"Description\", \"Population (GB+NI)\", \"Unemployment rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+\n",
      "|Description|Population (GB+NI)|Unemployment rate|\n",
      "+-----------+------------------+-----------------+\n",
      "|      Units|              000s|                %|\n",
      "|       1209|              null|             null|\n",
      "|       1210|              null|             null|\n",
      "|       1211|              null|             null|\n",
      "|       1212|              null|             null|\n",
      "|       1213|              null|             null|\n",
      "|       1214|              null|             null|\n",
      "|       1215|              null|             null|\n",
      "|       1216|              null|             null|\n",
      "|       1217|              null|             null|\n",
      "+-----------+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = data.select([\"Description\", \"Unemployment rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_description = unemployment.filter(unemployment['Description'] == 'Units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.join(other=cols_description, on=['Description'], how='left_anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.\\\n",
    "               withColumnRenamed(\"Description\", 'year').\\\n",
    "               withColumnRenamed(\"Population (GB+NI)\", \"population\").\\\n",
    "               withColumnRenamed(\"Unemployment rate\", \"unemployment_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|year|unemployment_rate|\n",
      "+----+-----------------+\n",
      "|1209|             null|\n",
      "|1210|             null|\n",
      "|1211|             null|\n",
      "|1212|             null|\n",
      "|1213|             null|\n",
      "|1214|             null|\n",
      "|1215|             null|\n",
      "|1216|             null|\n",
      "|1217|             null|\n",
      "|1218|             null|\n",
      "+----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment.write.saveAsTable('managed_table10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf= spark.sql(\"\"\"\n",
    "select * from managed_table10\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
