# Containerization of an Apache Spark Standalone Cluster on Docker

El objetivo de este repositorio es mostrar el uso de docker como una herramienta poderosa para desplegar aplicaciones que se comunican entre ellas de una manera rapida y estable, en este caso te presento un pequeño ecosistema de big data para procesamiento en memoria, el ecosistema esta basado en diferentes contenedores de docker, se tomo en consideracion agregar Hive y Hue como aplicaciones necesarias en un ecosistema simple de big data con apache spark, este proyecto te ayudarà a ir directamente a escribir codigo de apache spark y obtener experiencia practica y no preocuparte por la instalacion e implementacion de la infraestrcutura, a continuacion se detallan las aplicaciones instaladas y el porque de cada una:

- Apache Spark
- Hive
- JupyterLab
- namenode, datanode1 & datanode2
- Hue


Standalone cluster manager

# Docker containers architecture

still workin on it