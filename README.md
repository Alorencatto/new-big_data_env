# Containerization of an Apache Spark Standalone Cluster on Docker

The aim of this article is to show the use of docker as a powerful tool to deploy applications that communicate with each other in a fast and stable way, in this case I present a small ecosystem of big data for in-memory processing, the ecosystem is based in different docker containers, it was considered to add Apache Hive and Hue as needed applications in a simple big data ecosystem with Apache Spark, this project will help you go directly to writing scala or pyspark code and get hands-on experience and not worry about the installation and implementation of the infrastructure, the installed applications and the reason for each one them are detailed below.

- Apache Spark
- Hive
- JupyterLab
- namenode, datanode1 & datanode2
- Hue


Standalone cluster manager

# Docker containers architecture


![architecture](https://user-images.githubusercontent.com/8701464/127952650-c71d6374-3cb0-40fc-8df5-01ffda530081.png)


still workin on it
